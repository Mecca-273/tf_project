{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "uuid": "7fa175e5-312a-4750-a7af-8d0258e00fb8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "PROJECT = \"wals-vi\" # REPLACE WITH YOUR PROJECT ID\n",
    "BUCKET = \"wals-vi-ml\" # REPLACE WITH YOUR BUCKET NAME\n",
    "REGION = \"us-central1\" # REPLACE WITH YOUR BUCKET REGION e.g. us-central1\n",
    "\n",
    "# Do not change these\n",
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"BUCKET\"] = BUCKET\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"TFVERSION\"] = \"2.1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "uuid": "fbed52b8-59f4-4ac4-9933-fc0456d10e21"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'query' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-540517510333>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mwrite_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems_for_user\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOUTPUT_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"nusers\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_test_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'query' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import apache_beam as beam\n",
    "import tensorflow_transform as tft\n",
    "from tensorflow_transform.beam import impl as beam_impl\n",
    "\n",
    "def preprocess_tft(rowdict):\n",
    "    median = 57937 #tft.quantiles(rowdict[\"session_duration\"], 11, epsilon=0.001)[5]\n",
    "    result = {\n",
    "      \"userId\" : tft.string_to_int(rowdict[\"visitorId\"], vocab_filename=\"vocab_users\"),\n",
    "      \"itemId\" : tft.string_to_int(rowdict[\"contentId\"], vocab_filename=\"vocab_items\"),\n",
    "      \"rating\" : 0.3 * rowdict[\"session_duration\"] / median\n",
    "    }\n",
    "    # cap the rating at 1.0\n",
    "    result[\"rating\"] = tf.where(condition = tf.less(x = result[\"rating\"], y = tf.ones(shape = tf.shape(input = result[\"rating\"]))),\n",
    "                                x = result[\"rating\"], \n",
    "                                y = tf.ones(shape = tf.shape(input = result[\"rating\"])))\n",
    "    return result\n",
    "  \n",
    "def preprocess(query, in_test_mode):\n",
    "    import os\n",
    "    import os.path\n",
    "    import tempfile\n",
    "    import tensorflow as tf\n",
    "    from apache_beam.io import tfrecordio\n",
    "    from tensorflow_transform.coders import example_proto_coder\n",
    "    from tensorflow_transform.tf_metadata import dataset_metadata\n",
    "    from tensorflow_transform.tf_metadata import dataset_schema\n",
    "    from tensorflow_transform.beam.tft_beam_io import transform_fn_io\n",
    "\n",
    "    def write_count(a, outdir, basename):\n",
    "        filename = os.path.join(outdir, basename)\n",
    "        (a \n",
    "         | \"{}_1\".format(basename) >> beam.Map(lambda x: (1, 1)) \n",
    "         | \"{}_2\".format(basename) >> beam.combiners.Count.PerKey()\n",
    "         | \"{}_3\".format(basename) >> beam.Map(lambda k, v: v)\n",
    "         | \"{}_write\".format(basename) >> beam.io.WriteToText(file_path_prefix=filename, num_shards=1))\n",
    "\n",
    "    def to_tfrecord(key_vlist, indexCol):\n",
    "        (key, vlist) = key_vlist\n",
    "        return {\n",
    "            \"key\": [key],\n",
    "            \"indices\": [value[indexCol] for value in vlist],\n",
    "            \"values\":  [value[\"rating\"] for value in vlist]\n",
    "        }\n",
    "  \n",
    "    job_name = \"preprocess-wals-features\" + \"-\" + datetime.datetime.now().strftime(\"%y%m%d-%H%M%S\")    \n",
    "    if in_test_mode:\n",
    "        import shutil\n",
    "        print(\"Launching local job ... hang on\")\n",
    "        OUTPUT_DIR = \"./preproc_tft\"\n",
    "        shutil.rmtree(OUTPUT_DIR, ignore_errors=True)\n",
    "    else:\n",
    "        print(\"Launching Dataflow job {} ... hang on\".format(job_name))\n",
    "        OUTPUT_DIR = \"gs://{0}/wals/preproc_tft/\".format(BUCKET)\n",
    "        import subprocess\n",
    "        subprocess.call(\"gsutil rm -r {}\".format(OUTPUT_DIR).split())\n",
    "\n",
    "    options = {\n",
    "    \"staging_location\": os.path.join(OUTPUT_DIR, \"tmp\", \"staging\"),\n",
    "    \"temp_location\": os.path.join(OUTPUT_DIR, \"tmp\"),\n",
    "    \"job_name\": job_name,\n",
    "    \"project\": PROJECT,\n",
    "    \"max_num_workers\": 16,\n",
    "    \"teardown_policy\": \"TEARDOWN_ALWAYS\",\n",
    "    \"save_main_session\": False,\n",
    "    \"requirements_file\": \"requirements.txt\"\n",
    "    }\n",
    "    opts = beam.pipeline.PipelineOptions(flags=[], **options)\n",
    "    if in_test_mode:\n",
    "        RUNNER = \"DirectRunner\"\n",
    "    else:\n",
    "        RUNNER = \"DataflowRunner\"\n",
    "\n",
    "  # Set up metadata  \n",
    "    raw_data_schema = {\n",
    "        colname : dataset_schema.ColumnSchema(tf.string, [], dataset_schema.FixedColumnRepresentation()) \n",
    "            for colname in \"visitorId,contentId\".split(\",\")\n",
    "    }\n",
    "    raw_data_schema.update({\n",
    "        colname : dataset_schema.ColumnSchema(tf.float32, [], dataset_schema.FixedColumnRepresentation())\n",
    "            for colname in \"session_duration\".split(\",\")\n",
    "    })\n",
    "    raw_data_metadata = dataset_metadata.DatasetMetadata(dataset_schema.Schema(raw_data_schema))\n",
    " \n",
    "  # Run Beam  \n",
    "    with beam.Pipeline(RUNNER, options=opts) as p:\n",
    "        with beam_impl.Context(temp_dir=os.path.join(OUTPUT_DIR, \"tmp\")):\n",
    "            # read raw data\n",
    "            selquery = query\n",
    "            if in_test_mode:\n",
    "                 selquery = selquery + \" LIMIT 100\"\n",
    "            raw_data = (p \n",
    "                        | \"read\" >> beam.io.Read(beam.io.BigQuerySource(query=selquery, use_standard_sql=True)))\n",
    "    \n",
    "            # analyze and transform\n",
    "            raw_dataset = (raw_data, raw_data_metadata)\n",
    "            transformed_dataset, transform_fn = (\n",
    "                    raw_dataset | beam_impl.AnalyzeAndTransformDataset(preprocess_tft))         \n",
    "            transformed_data, transformed_metadata = transformed_dataset\n",
    "            _ = (transform_fn\n",
    "                 | \"WriteTransformFn\" >>\n",
    "                 transform_fn_io.WriteTransformFn(os.path.join(OUTPUT_DIR, \"transform_fn\")))\n",
    "            \n",
    "            # do a group-by to create users_for_item and items_for_user\n",
    "            users_for_item = (transformed_data \n",
    "                              | \"map_items\" >> beam.Map(lambda x : (x[\"itemId\"], x))\n",
    "                              | \"group_items\" >> beam.GroupByKey()\n",
    "                              | \"totfr_items\" >> beam.Map(lambda item_userlist : to_tfrecord(item_userlist, \"userId\")))\n",
    "            items_for_user = (transformed_data\n",
    "                              | \"map_users\" >> beam.Map(lambda x : (x[\"userId\"], x))\n",
    "                              | \"group_users\" >> beam.GroupByKey()\n",
    "                              | \"totfr_users\" >> beam.Map(lambda item_userlist : to_tfrecord(item_userlist, \"itemId\")))\n",
    "            \n",
    "            output_schema = {\n",
    "                \"key\" : dataset_schema.ColumnSchema(tf.int64, [1], dataset_schema.FixedColumnRepresentation()),\n",
    "                \"indices\": dataset_schema.ColumnSchema(tf.int64, [], dataset_schema.ListColumnRepresentation()),\n",
    "                \"values\": dataset_schema.ColumnSchema(tf.float32, [], dataset_schema.ListColumnRepresentation())\n",
    "            }\n",
    "\n",
    "            _ = users_for_item | \"users_for_item\" >> tfrecordio.WriteToTFRecord(\n",
    "                    os.path.join(OUTPUT_DIR, \"users_for_item\"),\n",
    "                    coder = example_proto_coder.ExampleProtoCoder(\n",
    "                            dataset_schema.Schema(output_schema)))\n",
    "            _ = items_for_user | \"items_for_user\" >> tfrecordio.WriteToTFRecord(\n",
    "                    os.path.join(OUTPUT_DIR, \"items_for_user\"),\n",
    "                    coder = example_proto_coder.ExampleProtoCoder(\n",
    "                            dataset_schema.Schema(output_schema)))\n",
    "            \n",
    "            write_count(users_for_item, OUTPUT_DIR, \"nitems\")\n",
    "            write_count(items_for_user, OUTPUT_DIR, \"nusers\") \n",
    "     \n",
    "preprocess(query, in_test_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "uuid": "7fff554f-b9af-433a-b9ce-63d92e50ad01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple, https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting tensorflow_transform\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/5d/9f/acad7dab38ba19f4c574de2b50ec13343fce6ac51c291b5fc81e59bc4466/tensorflow_transform-0.24.1-py3-none-any.whl (373 kB)\n",
      "\u001b[K     |████████████████████████████████| 373 kB 3.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pydot<2,>=1.2 in /home/admin/.local/lib/python3.6/site-packages (from tensorflow_transform) (1.4.1)\n",
      "Collecting tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<2.4,>=1.15.2\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/ad/ad/769c195c72ac72040635c66cd9ba7b0f4b4fc1ac67e59b99fa6988446c22/tensorflow-2.3.1-cp36-cp36m-manylinux2010_x86_64.whl (320.4 MB)\n",
      "\u001b[K     |██████▋                         | 66.3 MB 34.3 MB/s eta 0:00:08K     |█                               | 9.8 MB 82.6 MB/s eta 0:00:04��                              | 11.3 MB 82.6 MB/s eta 0:00:04040404                       | 22.8 MB 82.6 MB/s eta 0:00:04"
     ]
    }
   ],
   "source": [
    "pip install tensorflow_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "uuid": "5d98dd93-975a-4843-9429-a93d46c56c73"
   },
   "outputs": [],
   "source": [
    "pip install "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
